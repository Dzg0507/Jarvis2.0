{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Jarvis 2.0 - Free GPU Stable Diffusion Server\n",
        "\n",
        "This notebook runs your optimized Stable Diffusion server on Google Colab's free GPU (Tesla T4 - 16GB VRAM).\n",
        "\n",
        "**Benefits:**\n",
        "- ‚úÖ 16GB VRAM (vs your 4GB local GPU)\n",
        "- ‚úÖ Much faster generation\n",
        "- ‚úÖ No memory issues\n",
        "- ‚úÖ Completely free\n",
        "\n",
        "**Setup:**\n",
        "1. Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "2. Run all cells\n",
        "3. Copy the ngrok URL\n",
        "4. Update your local app to use the Colab URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install diffusers transformers accelerate xformers\n",
        "!pip install flask flask-cors\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup ngrok for public URL\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "# Get your ngrok token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "print(\"Get your free ngrok token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "ngrok_token = getpass.getpass(\"Enter your ngrok token: \")\n",
        "ngrok.set_auth_token(ngrok_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimized Stable Diffusion server\n",
        "server_code = '''\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Load pipeline with optimizations\n",
        "device = \"cuda\"\n",
        "logger.info(f\"Loading Stable Diffusion on {device}...\")\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,  # Use float16 on powerful GPU\n",
        "    safety_checker=None,\n",
        "    requires_safety_checker=False\n",
        ")\n",
        "\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# Enable all optimizations\n",
        "pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_attention_slicing()\n",
        "pipe.enable_vae_slicing()\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "logger.info(\"‚úÖ Pipeline loaded and optimized!\")\n",
        "\n",
        "@app.route(\"/health\", methods=[\"GET\"])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"healthy\", \"device\": device, \"gpu\": torch.cuda.get_device_name(0)})\n",
        "\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        prompt = data.get(\"prompt\", \"\")\n",
        "        negative_prompt = data.get(\"negative_prompt\", \"ugly, deformed, disfigured, poor details, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, mutated hands and fingers, disconnected limbs, mutation, mutated, disgusting, blurry, amputation, extra fingers, fewer fingers, extra hands, bad hands, sketches, lowres, normal quality, worstquality, signature, watermark, username, blurry, bad feet, cropped, poorly drawn hands, poorly drawn face, mutation, deformed, worst quality, low quality, jpeg artifacts, extra fingers, fewer digits, extra limbs, extra arms, extra legs, malformed limbs, fused fingers, too many fingers, long neck, mutated hands, bad body, bad proportions, gross proportions, text, error, missing fingers, missing arms, missing legs, extra digit, extra arms, extra leg, extra foot, bad face, asymmetric eyes, cross-eyed, uneven eyes, bad teeth, bad lips, bad nose, bad ears, bad hair, bad skin, scars, moles, wrinkles, old, elderly\")\n",
        "        width = data.get(\"width\", 768)  # Higher default resolution\n",
        "        height = data.get(\"height\", 768)  # Higher default resolution\n",
        "        steps = data.get(\"num_inference_steps\", 30)  # Higher quality default\n",
        "        guidance = data.get(\"guidance_scale\", 8.0)  # Better prompt adherence\n",
        "        seed = data.get(\"seed\")\n",
        "        \n",
        "        generator = None\n",
        "        if seed is not None:\n",
        "            generator = torch.Generator(device=device).manual_seed(seed)\n",
        "        \n",
        "        logger.info(f\"Generating: {prompt} ({width}x{height}, {steps} steps)\")\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            result = pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                width=width,\n",
        "                height=height,\n",
        "                num_inference_steps=steps,\n",
        "                guidance_scale=guidance,\n",
        "                generator=generator\n",
        "            )\n",
        "        \n",
        "        image = result.images[0]\n",
        "        \n",
        "        # Convert to base64\n",
        "        buffer = io.BytesIO()\n",
        "        image.save(buffer, format=\"PNG\")\n",
        "        img_str = base64.b64encode(buffer.getvalue()).decode()\n",
        "        \n",
        "        logger.info(f\"‚úÖ Generated successfully! Size: {len(img_str)} chars\")\n",
        "        \n",
        "        return jsonify({\n",
        "            \"success\": True,\n",
        "            \"image\": f\"data:image/png;base64,{img_str}\",\n",
        "            \"prompt\": prompt,\n",
        "            \"seed\": seed,\n",
        "            \"device\": device\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Generation failed: {e}\")\n",
        "        return jsonify({\"success\": False, \"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=5000)\n",
        "'''\n",
        "\n",
        "with open('colab_sd_server.py', 'w') as f:\n",
        "    f.write(server_code)\n",
        "\n",
        "print(\"‚úÖ Server code created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the server with ngrok tunnel\n",
        "import threading\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Start server in background\n",
        "def run_server():\n",
        "    subprocess.run([\"python\", \"colab_sd_server.py\"])\n",
        "\n",
        "server_thread = threading.Thread(target=run_server)\n",
        "server_thread.daemon = True\n",
        "server_thread.start()\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(10)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"\\nüéâ Your Colab Stable Diffusion server is running!\")\n",
        "print(f\"üìç Public URL: {public_url}\")\n",
        "print(f\"üè• Health check: {public_url}/health\")\n",
        "print(f\"üñºÔ∏è Generate endpoint: {public_url}/generate\")\n",
        "print(f\"\\nüìã Copy this URL and update your local app!\")\n",
        "\n",
        "# Keep the server running\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "        print(f\"‚è∞ Server still running: {public_url}\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüõë Server stopped\")\n",
        "    ngrok.disconnect(public_url)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
